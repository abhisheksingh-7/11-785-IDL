{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# HW4P2: Attention-based Speech Recognition\n",
        "\n",
        "<img src=\"https://cdn.shopify.com/s/files/1/0272/2080/3722/products/SmileBumperSticker_5400x.jpg\" alt=\"A cute cat\" width=\"600\">\n",
        "\n",
        "\n",
        "Welcome to the final assignment in 11785. In this HW, you will work on building a speech recognition system with <i>attention</i>. <br> <br>\n",
        "\n",
        "<center>\n",
        "<img src=\"https://popmn.org/wp-content/uploads/2020/03/pay-attention.jpg\" alt=\"A cute cat\" height=\"100\">\n",
        "</center>\n",
        "\n",
        "HW Writeup: On Piazza/Course Website <br>\n",
        "Kaggle Competition Link: https://www.kaggle.com/competitions/11-785-s23-hw4p2/ <br>\n",
        "Kaggle Dataset Link: https://www.kaggle.com/datasets/varunjain3/11-785-s23-hw4p2-dataset\n",
        "<br>\n",
        "LAS Paper: https://arxiv.org/pdf/1508.01211.pdf <br>\n",
        "Attention is all you need:https://arxiv.org/pdf/1706.03762.pdf"
      ],
      "metadata": {
        "id": "8XpNMS7Vk6Df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read this section importantly!"
      ],
      "metadata": {
        "id": "vwIdDTTmmZVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. By now, we believe that you are already a great deep learning practitioner, Congratulations. ðŸŽ‰\n",
        "\n",
        "2. You are allowed to use code from your previous homeworks for this homework. We will only provide, aspects that are necessary and new with this homework. \n",
        "\n",
        "3. There are a lot of resources provided in this notebook, that will help you check if you are running your implementations correctly."
      ],
      "metadata": {
        "id": "y9qsVrRemgh7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8UK7J-dp5iN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install some required libraries\n",
        "# Feel free to add more if you want\n",
        "!pip install -q python-levenshtein torchsummaryX wandb kaggle pytorch-nlp "
      ],
      "metadata": {
        "id": "nYgaLmgy5iqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "yEkA_GGG-tTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Necessary Modules you require for this HW here"
      ],
      "metadata": {
        "id": "p0aXmrxM-usO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Toy Dataset Download"
      ],
      "metadata": {
        "id": "h7Rd-7SJEKX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://cmu.box.com/shared/static/om4qpzd4tf1xo4h7230k4v1pbdyueghe --content-disposition --show-progress\n",
        "!unzip -q hw4p2_toy.zip -d ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emvEV0KwEKDX",
        "outputId": "21369b6f-e518-41d6-c64b-e0b2ea817348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hw4p2_toy.zip       100%[===================>] 343.92M  12.1MB/s    in 29s     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggle Dataset Download"
      ],
      "metadata": {
        "id": "-njBvl2Opd6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_token = '{\"username\":\"\",\"key\":\"\"}'\n",
        "\n",
        "# set up kaggle.json\n",
        "# TODO: Use the same Kaggle code from HW1P2, HW2P2, HW3P2\n",
        "!mkdir /root/.kaggle/\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write(api_token) # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "PTyWR2sIp0Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To download the dataset\n",
        "!kaggle datasets download -d varunjain3/11-785-s23-hw4p2-dataset"
      ],
      "metadata": {
        "id": "F581gjfnqE2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To unzip data quickly and quietly\n",
        "!unzip -q 11-785-s23-hw4p2-dataset.zip -d ./data"
      ],
      "metadata": {
        "id": "7ko7QN16qF2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Dataloaders\n",
        "\n",
        "We have given you 2 datasets. One is a toy dataset, and the other is the standard LibriSpeech dataset. The toy dataset is to help you get your code implemented and tested and debugged easily, to verify that your attention diagonal is produced correctly. Note however that it's task (phonetic transcription) is drawn from HW3P2, it is meant to be familiar and help you understand how to transition from phonetic transcription to alphabet transcription, with a working attention module.\n",
        "\n",
        "Please make sure you use the right constants in your code implementation for future modules, (SOS_TOKEN vs SOS_TOKEN_TOY) when working with either dataset. We have defined the constants accordingly below. Before you come to OH or post on piazza, make sure you aren't misuing the constants for either dataset in your code. "
      ],
      "metadata": {
        "id": "zUJyBBwIqQs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Toy Dataset"
      ],
      "metadata": {
        "id": "G_8wdpm3WrT9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The toy dataset is a dataset of fixed length speech sequences that have phonetic transcripts. The reason we made it with phonetic transcripts was to help you understand how attention can work with phonetic transcription that you have done in HW3P2"
      ],
      "metadata": {
        "id": "kCzEdHVZWyga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the toy dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "X_train = np.load(\"hw4p2_toy/f0176_mfccs_train_new.npy\")\n",
        "X_valid = np.load(\"hw4p2_toy/f0176_mfccs_dev_new.npy\")\n",
        "Y_train = np.load(\"hw4p2_toy/f0176_hw3p2_train.npy\")\n",
        "Y_valid = np.load(\"hw4p2_toy/f0176_hw3p2_dev.npy\")\n",
        "\n",
        "# This is how you actually need to find out the different trancripts in a dataset. \n",
        "# Can you think whats going on here? Why are we using a np.unique?\n",
        "VOCAB_MAP_TOY           = dict(zip(np.unique(Y_valid), range(len(np.unique(Y_valid))))) \n",
        "VOCAB_MAP_TOY[\"[PAD]\"]  = len(VOCAB_MAP_TOY)\n",
        "VOCAB_TOY               = list(VOCAB_MAP_TOY.keys())\n",
        "\n",
        "SOS_TOKEN_TOY = VOCAB_MAP_TOY[\"[SOS]\"]\n",
        "EOS_TOKEN_TOY = VOCAB_MAP_TOY[\"[EOS]\"]\n",
        "PAD_TOKEN_TOY = VOCAB_MAP_TOY[\"[PAD]\"]\n",
        "\n",
        "Y_train = [np.array([VOCAB_MAP_TOY[p] for p in seq]) for seq in Y_train]\n",
        "Y_valid = [np.array([VOCAB_MAP_TOY[p] for p in seq]) for seq in Y_valid]"
      ],
      "metadata": {
        "id": "oLhnu3Y2WvFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToyDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, partition):\n",
        "\n",
        "        if partition == \"train\":\n",
        "            self.mfccs = X_train\n",
        "            self.transcripts = Y_train\n",
        "\n",
        "        elif partition == \"valid\":\n",
        "            self.mfccs = X_valid\n",
        "            self.transcripts = Y_valid\n",
        "\n",
        "        assert len(self.mfccs) == len(self.transcripts)\n",
        "\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "\n",
        "        x = torch.tensor(self.mfccs[i])\n",
        "        y = torch.tensor(self.transcripts[i])\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        x_batch, y_batch = list(zip(*batch))\n",
        "\n",
        "        x_lens      = [x.shape[0] for x in x_batch] \n",
        "        y_lens      = [y.shape[0] for y in y_batch] \n",
        "\n",
        "        x_batch_pad = torch.nn.utils.rnn.pad_sequence(x_batch, batch_first=True, padding_value= EOS_TOKEN_TOY)\n",
        "        y_batch_pad = torch.nn.utils.rnn.pad_sequence(y_batch, batch_first=True, padding_value= EOS_TOKEN_TOY) \n",
        "        \n",
        "        return x_batch_pad, y_batch_pad, torch.tensor(x_lens), torch.tensor(y_lens)"
      ],
      "metadata": {
        "id": "U5ub4PLbWsaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {}\n",
        "config['batch_size'] = 128\n",
        "train_toy_dataset   = ToyDataset(partition= 'train')\n",
        "valid_toy_dataset   = ToyDataset(partition= 'valid')\n",
        "\n",
        "train_toy_loader    = torch.utils.data.DataLoader(\n",
        "    dataset     = train_toy_dataset, \n",
        "    batch_size  = config['batch_size'], \n",
        "    shuffle     = True,\n",
        "    num_workers = 4, \n",
        "    pin_memory  = True,\n",
        "    collate_fn  = train_toy_dataset.collate_fn\n",
        ")\n",
        "\n",
        "valid_toy_loader    = torch.utils.data.DataLoader(\n",
        "    dataset     = valid_toy_dataset, \n",
        "    batch_size  = config['batch_size'], \n",
        "    shuffle     = False,\n",
        "    num_workers = 2, \n",
        "    pin_memory  = True,\n",
        "    collate_fn  = valid_toy_dataset.collate_fn\n",
        ")\n",
        "\n",
        "print(\"No. of train mfccs   : \", train_toy_dataset.__len__())\n",
        "print(\"Batch size           : \", config['batch_size'])\n",
        "print(\"Train batches        : \", train_toy_loader.__len__())\n",
        "print(\"Valid batches        : \", valid_toy_loader.__len__())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KSpirO0MZnON",
        "outputId": "34965b35-3e38-4a0f-de27-630bec887333"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of train mfccs   :  16000\n",
            "Batch size           :  128\n",
            "Train batches        :  125\n",
            "Valid batches        :  13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LibriSpeech\n",
        "\n",
        "In terms of the dataset, the dataset structure for HW3P2 and HW4P2 dataset are very similar. Can you spot out the differences? What all will be required?? \n",
        "\n",
        "Hints:\n",
        "\n",
        "- Check how big is the dataset (do you require memory efficient loading techniques??)\n",
        "- How do we load mfccs? Do we need to normalise them? \n",
        "- Does the data have \\<SOS> and \\<EOS> tokens in each sequences? Do we remove them or do we not remove them? (Read writeup)\n",
        "- Would we want a collating function? Ask yourself: Why did we need a collate function last time?\n",
        "- Observe the VOCAB, is the dataset same as HW3P2? \n",
        "- Should you add augmentations, if yes which augmentations? When should you add augmentations? (Check bootcamp for answer)\n"
      ],
      "metadata": {
        "id": "1sjsbFuKWp7q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBMLGYX-kZcd",
        "outputId": "90d67714-0574-4557-e9f3-15a4fcf8a7de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocab: 31\n",
            "Vocab: ['<pad>', '<sos>', '<eos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ']\n",
            "PAD_TOKEN: 0\n",
            "SOS_TOKEN: 1\n",
            "EOS_TOKEN: 2\n"
          ]
        }
      ],
      "source": [
        "config = {\n",
        "  'batch_size': 128,\n",
        "  'lr':1e-4,\n",
        "  'epochs': 30,\n",
        "}\n",
        "\n",
        "VOCAB = ['<pad>', '<sos>', '<eos>', \n",
        "         'A',   'B',    'C',    'D',    \n",
        "         'E',   'F',    'G',    'H',    \n",
        "         'I',   'J',    'K',    'L',       \n",
        "         'M',   'N',    'O',    'P',    \n",
        "         'Q',   'R',    'S',    'T', \n",
        "         'U',   'V',    'W',    'X', \n",
        "         'Y',   'Z',    \"'\",    ' ', \n",
        "         ]\n",
        "\n",
        "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
        "\n",
        "PAD_TOKEN = VOCAB_MAP[\"<pad>\"]\n",
        "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
        "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
        "\n",
        "print(f\"Length of vocab: {len(VOCAB)}\")\n",
        "print(f\"Vocab: {VOCAB}\")\n",
        "print(f\"PAD_TOKEN: {PAD_TOKEN}\")\n",
        "print(f\"SOS_TOKEN: {SOS_TOKEN}\")\n",
        "print(f\"EOS_TOKEN: {EOS_TOKEN}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class SpeechDataset(torch.utils.data.Dataset):\n",
        "  '''\n",
        "  Feel free to add arguments, additional functions, this is the \n",
        "  bare-minimum template.\n",
        "  '''\n",
        "  def __init__(self,):\n",
        "    pass\n",
        "  \n",
        "  def __length__(self,):\n",
        "    pass\n",
        "  \n",
        "  def __getitem__(self,):\n",
        "    pass\n",
        "  \n",
        "  def collate_fn(self,):\n",
        "    pass"
      ],
      "metadata": {
        "id": "VuneWaTStdF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev_dataset = \n",
        "train_dataset = \n",
        "test_dataset = \n",
        "\n",
        "dev_loader = \n",
        "train_loader = \n",
        "test_loader = \n",
        "\n",
        "print(\"\\nChecking the shapes of the data...\")\n",
        "for batch in dev_loader:\n",
        "    x, y, x_len, y_len = batch\n",
        "    print(x.shape, y.shape, x_len.shape, y_len.shape)\n",
        "    break"
      ],
      "metadata": {
        "id": "tzuIXCyAuNvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check if you are loading the data correctly with the following:\n",
        "\n",
        "(Note: These are outputs from loading your data in the dataset class, not your dataloader which will have padded sequences)\n",
        "\n",
        "- Train Dataset\n",
        "```\n",
        "Partition loaded:  train-clean-100\n",
        "Max mfcc length:  2448\n",
        "Average mfcc length:  1264.6258453344547\n",
        "Max transcript:  400\n",
        "Average transcript length:  186.65321139493324\n",
        "```\n",
        "\n",
        "- Dev Dataset\n",
        "```\n",
        "Partition loaded:  dev-clean\n",
        "Max mfcc length:  3260\n",
        "Average mfcc length:  713.3570107288198\n",
        "Max transcript:  518\n",
        "Average transcript length:  108.71698113207547\n",
        "```\n",
        "\n",
        "- Test Dataset\n",
        "```\n",
        "Partition loaded:  test-clean\n",
        "Max mfcc length:  3491\n",
        "Average mfcc length:  738.2206106870229\n",
        "```\n",
        "\n",
        "If your values is not matching, read hints, think what could have gone wrong. Then approach TAs."
      ],
      "metadata": {
        "id": "i_n3pqt7ud4t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# THE MODEL \n",
        "\n",
        "### Listen, Attend and Spell\n",
        "Listen, Attend and Spell (LAS) is a neural network model used for speech recognition and synthesis tasks.\n",
        "\n",
        "- LAS is designed to handle long input sequences and is robust to noisy speech signals.\n",
        "- LAS is known for its high accuracy and ability to improve over time with additional training data.\n",
        "- It consists of an <b>listener, an attender and a speller</b>, which work together to convert an input speech signal into a corresponding output text.\n",
        "\n",
        "#### The Dataflow:\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/DataFlow.png\" alt=\"data flow\" height=\"100\">\n",
        "</center>\n",
        "\n",
        "#### The Listener: \n",
        "- converts the input speech signal into a sequence of hidden states.\n",
        "\n",
        "#### The Attender:\n",
        "- Decides how the sequence of Encoder hidden state is propogated to decoder.\n",
        "\n",
        "#### The Speller:\n",
        "- A language model, that incorporates the \"context of attender\"(output of attender) to predict sequence of words.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8q9wt4TwzPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Listener:"
      ],
      "metadata": {
        "id": "aPL08W_Z3R18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Psuedocode:\n",
        "```python\n",
        "class Listner:\n",
        "  def init():\n",
        "    feature_embedder = #Few layers of 1DConv-batchnorm-activation (Don't overdo)\n",
        "    pblstm_encoder = #Cascaded pblstm layers (Take pblstm from #HW3P2), \n",
        "    #can add more sequential lstms \n",
        "    dropout = #As per your liking\n",
        "\n",
        "  def forward(x,lx):\n",
        "    embedding = feature_embedder(x) #optional\n",
        "    encoding, encoding_len = pblstm_encoder(embedding/x,lx)\n",
        "    #Regularization if needed\n",
        "    return encoding, encoding_len\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "_ewKlVQF3edC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Listener(torch.nn.Module):\n",
        "  def __init__(self, ):\n",
        "    super().__init__()\n",
        "    pass\n",
        "  \n",
        "  def forward(self, x, lx):\n",
        "    pass"
      ],
      "metadata": {
        "id": "jTQHiB1jvM6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention\n",
        "\n",
        "### Different ways to compute Attention\n",
        "\n",
        "1. Dot-product attention\n",
        "    * raw_weights = bmm(key, query) \n",
        "    * Optional: Scaled dot-product by normalizing with sqrt key dimension \n",
        "    * Check \"Attention is All You Need\" Section 3.2.1\n",
        "    * 1st way is what most TAs are comfortable with, but if you want to explore, check out other methods below\n",
        "\n",
        "\n",
        "2. Cosine attention\n",
        "    * raw_weights = cosine(query, key) # almost the same as dot-product xD \n",
        "\n",
        "3. Bi-linear attention\n",
        "    * W = Linear transformation (learnable parameter): d_k -> d_q\n",
        "    * raw_weights = bmm(key @ W, query)\n",
        "\n",
        "4. Multi-layer perceptron\n",
        "    * Check \"Neural Machine Translation and Sequence-to-sequence Models: A Tutorial\" Section 8.4\n",
        "\n",
        "5. Multi-Head Attention\n",
        "    * Check \"Attention is All You Need\" Section 3.2.2\n",
        "    * h = Number of heads\n",
        "    * W_Q, W_K, W_V: Weight matrix for Q, K, V (h of them in total)\n",
        "    * W_O: d_v -> d_v\n",
        "    * Reshape K: (B, T, d_k) to (B, T, h, d_k // h) and transpose to (B, h, T, d_k // h)\n",
        "    * Reshape V: (B, T, d_v) to (B, T, h, d_v // h) and transpose to (B, h, T, d_v // h)\n",
        "    * Reshape Q: (B, d_q) to (B, h, d_q // h) `\n",
        "    * raw_weights = Q @ K^T\n",
        "    * masked_raw_weights = mask(raw_weights)\n",
        "    * attention = softmax(masked_raw_weights)\n",
        "    * multi_head = attention @ V\n",
        "    * multi_head = multi_head reshaped to (B, d_v)\n",
        "    * context = multi_head @ W_O"
      ],
      "metadata": {
        "id": "5fG9jDZBVklL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pseudocode:\n",
        "\n",
        "```python\n",
        "class Attention:\n",
        "    '''\n",
        "    Attention is calculated using the key, value (from encoder embeddings) and query from decoder.\n",
        "\n",
        "    After obtaining the raw weights, compute and return attention weights and context as follows.:\n",
        "\n",
        "    attention_weights   = softmax(raw_weights)\n",
        "    attention_context   = einsum(\"thinkwhatwouldbetheequationhere\",attention, value) #take hint from raw_weights calculation\n",
        "\n",
        "    At the end, you can pass context through a linear layer too.\n",
        "    '''\n",
        "\n",
        "    def init(listener_hidden_size,\n",
        "              speller_hidden_size,\n",
        "              projection_size):\n",
        "\n",
        "        VW = Linear(listener_hidden_size,projection_size)\n",
        "        KW = Linear(listener_hidden_size,projection_size)\n",
        "        QW = Linear(speller_hidden_size,projection_size)\n",
        "\n",
        "    def set_key_value(encoder_outputs):\n",
        "        '''\n",
        "        In this function we take the encoder embeddings and make key and values from it.\n",
        "        key.shape   = (batch_size, timesteps, projection_size)\n",
        "        value.shape = (batch_size, timesteps, projection_size)\n",
        "        '''\n",
        "        key = KW(encoder_outputs)\n",
        "        value = VW(encoder_outputs)\n",
        "      \n",
        "    def compute_context(decoder_context):\n",
        "        '''\n",
        "        In this function from decoder context, we make the query, and then we\n",
        "         multiply the queries with the keys to find the attention logits, \n",
        "         finally we take a softmax to calculate attention energy which gets \n",
        "         multiplied to the generted values and then gets summed.\n",
        "\n",
        "        key.shape   = (batch_size, timesteps, projection_size)\n",
        "        value.shape = (batch_size, timesteps, projection_size)\n",
        "        query.shape = (batch_size, projection_size)\n",
        "\n",
        "        You are also recomended to check out Abu's Lecture 19 to understand Attention better.\n",
        "        '''\n",
        "        query = QW(decoder_context) #(batch_size, projection_size)\n",
        "\n",
        "        raw_weights = #using bmm or einsum. We need to perform batch matrix multiplication. It is important you do this step correctly.\n",
        "        #What will be the shape of raw_weights?\n",
        "\n",
        "        attention_weights = #What makes raw_weights -> attention_weights\n",
        "\n",
        "        attention_context = #Multiply attention weights to values\n",
        "\n",
        "        return attention_context, attention_weights \n",
        "```"
      ],
      "metadata": {
        "id": "QyO03fO5VotY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    pass\n",
        "  \n",
        "  def set_key_value(self, encoder_outputs):\n",
        "    pass\n",
        "\n",
        "  def compute_context(self, decoder_context):\n",
        "    pass"
      ],
      "metadata": {
        "id": "771TXxn7ViOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The Speller\n",
        "\n",
        "Similar to the language model that you coded up for HW4P1, you have to code a language model for HW4P2 as well. This time, we will also call the attention context step, within the decoder to get the attended-encoder-embeddings.\n",
        "\n",
        "\n",
        "What you have coded till now:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/EncoderAttention.png\" alt=\"data flow\" height=\"400\">\n",
        "</center>\n",
        "\n",
        "For the Speller, what we have to code:\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://github.com/varunjain3/11785_s23_h4p2/raw/main/Decoder.png\" alt=\"data flow\" height=\"400\">\n",
        "</center>"
      ],
      "metadata": {
        "id": "4Sp1WywZmm1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Speller(torch.nn.Module):\n",
        "\n",
        "  # Refer to your HW4P1 implementation for help with setting up the language model.\n",
        "  # The only thing you need to implement on top of your HW4P1 model is the attention module and teacher forcing.\n",
        "\n",
        "  def __init__(self, attender:Attention):\n",
        "    super(). __init__()\n",
        "\n",
        "    self.attend = attender # Attention object in speller\n",
        "    self.max_timesteps = # Max timesteps\n",
        "\n",
        "    self.embedding =  # Embedding layer to convert token to latent space\n",
        "    self.lstm_cells =  # Create a sequence of LSTM Cells\n",
        "    \n",
        "    # For CDN (Feel free to change)\n",
        "    self.output_to_char = # Linear module to convert outputs to correct hidden size (Optional: TO make dimensions match)\n",
        "    self.activation = # Check which activation is suggested\n",
        "    self.char_prob = # Linear layer to convert hidden space back to logits for token classification\n",
        "    self.char_prob.weight = # Weight tying (From embedding layer)\n",
        "\n",
        "\n",
        "  def lstm_step(self, input_word, hidden_state):\n",
        "\n",
        "    for i in range(len(self.lstm_cells)):\n",
        "        raise NotImplementedError # Feed the input through each LSTM Cell\n",
        "  \n",
        "    return ... # What information does forward() need?\n",
        "    \n",
        "  def CDN(self,):\n",
        "    # Make the CDN here, you can add the output-to-char\n",
        "    raise NotImplementedError\n",
        "    \n",
        "  def forward (self, y=None, teacher_forcing_ratio=1):\n",
        "\n",
        "    attn_context = # initial context tensor for time t = 0\n",
        "    output_symbol = # Set it to SOS for time t = 0\n",
        "    raw_outputs = []  \n",
        "    attention_plot = []\n",
        "      \n",
        "    if y is None:\n",
        "      timesteps = self.max_timesteps\n",
        "      teacher_forcing_ratio = 0 #Why does it become zero?\n",
        "\n",
        "    else:\n",
        "      timesteps = raise NotImplementedError # How many timesteps are we predicting for?\n",
        "\n",
        "    hidden_states_list = # Initialize your hidden_states list here similar to HW4P1\n",
        "\n",
        "    for t in range(timesteps):\n",
        "      p = # generate a probability p between 0 and 1\n",
        "\n",
        "      if p < teacher_forcing_ratio and t > 0: # Why do we consider cases only when t > 0? What is considered when t == 0? Think.\n",
        "        output_symbol = # Take from y, else draw from probability distribution\n",
        "\n",
        "\n",
        "      char_embed = raise NotImplementedError # Embed the character symbol\n",
        "\n",
        "      # Concatenate the character embedding and context from attention, as shown in the diagram\n",
        "      lstm_input = raise NotImplementedError\n",
        "\n",
        "      ... = self.lstm_step(...) # Feed the input through LSTM Cells and attention.\n",
        "      # What should we retrieve from forward_step to prepare for the next timestep?\n",
        "\n",
        "      attn_context, attn_weights = self.attend.compute_context(...) # Feed the resulting hidden state into attention\n",
        "\n",
        "      cdn_input = # TODO: You need to concatenate the context from the attention module with the LSTM output hidden state, as shown in the diagram\n",
        "\n",
        "      raw_pred = raise NotImplementedError # call CDN with cdn_input\n",
        "\n",
        "      # Generate a prediction for this timestep and collect it in output_symbols\n",
        "      output_symbol = # Draw correctly from raw_pred\n",
        "\n",
        "      raw_outputs.append(raw_pred) # for loss calculation\n",
        "      attention_plot.append(attn_weights) # for plotting attention plot\n",
        "\n",
        "    \n",
        "    attention_plot = torch.stack(attention_plot, dim=1)\n",
        "    raw_outputs = torch.stack(raw_outputs, dim=1)\n",
        "\n",
        "    return raw_outputs, attention_plot"
      ],
      "metadata": {
        "id": "nFkc6MbnlUPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LAS"
      ],
      "metadata": {
        "id": "bgOQlDRI-E4z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we finally build the LAS model, comibining the listener, attender and speller together, we have given a template, but you are free to read the paper and implement it yourself."
      ],
      "metadata": {
        "id": "ZuAjQFlTBVED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LAS(torch.nn.Module):\n",
        "  def __init__(self,): # add parameters\n",
        "    super().__init__()\n",
        "\n",
        "    # Pass the right parameters here\n",
        "    self.listener = Listener()\n",
        "    self.attend = Attention()\n",
        "    self.speller = Speller(self.attend)\n",
        "\n",
        "  def forward(self, x,lx,y=None,teacher_forcing_ratio=1):\n",
        "    # Encode speech features\n",
        "    encoder_outputs, _ = self.listener(x,lx)\n",
        "\n",
        "    # We want to compute keys and values ahead of the decoding step, as they are constant for all timesteps\n",
        "    # Set keys and values using the encoder outputs\n",
        "    self.attend.set_key_value(encoder_outputs)\n",
        "\n",
        "    # Decode text with the speller using context from the attention\n",
        "    raw_outputs, attention_plots = self.speller(y=y,teacher_forcing_ratio=teacher_forcing_ratio)\n",
        "\n",
        "    return raw_outputs, attention_plots"
      ],
      "metadata": {
        "id": "scvB2cI-OSof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Setup "
      ],
      "metadata": {
        "id": "bPZD3vqdUisj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Baseline LAS has the following configuration:\n",
        "# Encoder bLSTM/pbLSTM Hidden Dimension of 512 (256 per direction)\n",
        "# Decoder Embedding Layer Dimension of 256\n",
        "# Decoder Hidden Dimension of 512 \n",
        "# Attention Projection Size of 128\n",
        "# Feel Free to Experiment with this \n",
        "\n",
        "model = LAS(\n",
        "    # Initialize your model \n",
        "    # Read the paper and think about what dimensions should be used\n",
        "    # You can experiment on these as well, but they are not requried for the early submission\n",
        "    # Remember that if you are using weight tying, some sizes need to be the same\n",
        ")\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "print(model)\n",
        "\n",
        "summary(model, \n",
        "        x, \n",
        "        lx, \n",
        "        y)"
      ],
      "metadata": {
        "id": "a9LN0l5VUk_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function, Optimizers, Scheduler"
      ],
      "metadata": {
        "id": "23DMfXsaU6kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer   = torch.optim.Adam(model.parameters(), lr= config['lr']) # Feel free to experiment if needed\n",
        "criterion   = torch.nn.CrossEntropyLoss(reduction='?',ignore_index='?') #check how would you fill these values : https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "scaler      = torch.cuda.amp.GradScaler()\n",
        "scheduler   = ...\n",
        "\n",
        "# Optional (but Recommended): Create a custom class for a Teacher Force Schedule"
      ],
      "metadata": {
        "id": "216ukmHbU-ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Levenshtein Distance"
      ],
      "metadata": {
        "id": "ZWQnB8lUVY4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n",
        "def indices_to_chars(indices, vocab):\n",
        "    tokens = []\n",
        "    for i in indices: # This loops through all the indices\n",
        "        if int(i) == SOS_TOKEN: # If SOS is encountered, dont add it to the final list\n",
        "            continue\n",
        "        elif int(i) == EOS_TOKEN: # If EOS is encountered, stop the decoding process\n",
        "            break\n",
        "        else:\n",
        "            tokens.append(vocab[int(i)])\n",
        "    return tokens\n",
        "\n",
        "# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n",
        "def calc_edit_distance(predictions, y, ly, vocab= VOCAB, print_example= False):\n",
        "\n",
        "    dist                = 0\n",
        "    batch_size, seq_len = predictions.shape\n",
        "\n",
        "    for batch_idx in range(batch_size): \n",
        "\n",
        "        y_sliced    = indices_to_chars(y[batch_idx,0:ly[batch_idx]], vocab)\n",
        "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
        "\n",
        "        # Strings - When you are using characters from the AudioDataset\n",
        "        y_string    = ''.join(y_sliced)\n",
        "        pred_string = ''.join(pred_sliced)\n",
        "        \n",
        "        dist        += Levenshtein.distance(pred_string, y_string)\n",
        "        # Comment the above and uncomment below for toy dataset, as the toy dataset has a list of phonemes to compare\n",
        "        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
        "\n",
        "    if print_example: \n",
        "        # Print y_sliced and pred_sliced if you are using the toy dataset\n",
        "        print(\"Ground Truth : \", y_string)\n",
        "        print(\"Prediction   : \", pred_string)\n",
        "        \n",
        "    dist/=batch_size\n",
        "    return dist"
      ],
      "metadata": {
        "id": "rSsiCdxPVeZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train and Validation functions \n"
      ],
      "metadata": {
        "id": "Pu4MrSMUUIyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, criterion, optimizer, teacher_forcing_rate):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    running_loss        = 0.0\n",
        "    running_perplexity  = 0.0\n",
        "    \n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "\n",
        "            raw_predictions, attention_plot = model(x, lx, y= y, tf_rate= teacher_forcing_rate)\n",
        "\n",
        "            # Predictions are of Shape (batch_size, timesteps, vocab_size). \n",
        "            # Transcripts are of shape (batch_size, timesteps) Which means that you have batch_size amount of batches with timestep number of tokens.\n",
        "            # So in total, you have batch_size*timesteps amount of characters.\n",
        "            # Similarly, in predictions, you have batch_size*timesteps amount of probability distributions.\n",
        "            # How do you need to modify transcipts and predictions so that you can calculate the CrossEntropyLoss? Hint: Use Reshape/View and read the docs\n",
        "            # Also we recommend you plot the attention weights, you should get convergence in around 10 epochs, if not, there could be something wrong with \n",
        "            # your implementation\n",
        "            loss        =  # TODO: Cross Entropy Loss\n",
        "\n",
        "            perplexity  = torch.exp(loss) # Perplexity is defined the exponential of the loss\n",
        "\n",
        "            running_loss        += loss.item()\n",
        "            running_perplexity  += perplexity.item()\n",
        "        \n",
        "        # Backward on the masked loss\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Optional: Use torch.nn.utils.clip_grad_norm to clip gradients to prevent them from exploding, if necessary\n",
        "        # If using with mixed precision, unscale the Optimizer First before doing gradient clipping\n",
        "        \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        \n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(running_loss/(i+1)),\n",
        "            perplexity=\"{:.04f}\".format(running_perplexity/(i+1)),\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])),\n",
        "            tf_rate='{:.02f}'.format(teacher_forcing_rate))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    running_loss /= len(dataloader)\n",
        "    running_perplexity /= len(dataloader)\n",
        "    batch_bar.close()\n",
        "\n",
        "    return running_loss, running_perplexity, attention_plot"
      ],
      "metadata": {
        "id": "sKOdI0J5Tpem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, dataloader):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc=\"Val\")\n",
        "\n",
        "    running_lev_dist = 0.0\n",
        "\n",
        "    for i, (x, y, lx, ly) in enumerate(dataloader):\n",
        "\n",
        "        x, y, lx, ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            raw_predictions, attentions = model(x, lx, y = None)\n",
        "\n",
        "        # Greedy Decoding\n",
        "        greedy_predictions   =  # TODO: How do you get the most likely character from each distribution in the batch?\n",
        "\n",
        "        # Calculate Levenshtein Distance\n",
        "        running_lev_dist    += calc_edit_distance(greedy_predictions, y, ly, VOCAB, print_example = False) # You can use print_example = True for one specific index i in your batches if you want\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    running_lev_dist /= len(dataloader)\n",
        "\n",
        "    return running_lev_dist"
      ],
      "metadata": {
        "id": "YmBLhP8cWm6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment"
      ],
      "metadata": {
        "id": "JmZhxhNseaIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Login to Wandb\n",
        "# Initialize your Wandb Run Here\n",
        "# Save your model architecture in a txt file, and save the file to Wandb"
      ],
      "metadata": {
        "id": "sZcCV2BIW2R6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_attention(attention): \n",
        "    # Function for plotting attention\n",
        "    # You need to get a diagonal plot\n",
        "    plt.clf()\n",
        "    sns.heatmap(attention, cmap='GnBu')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IgJdA9jrZwid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_lev_dist = float(\"inf\")\n",
        "tf_rate = 1.0\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "    \n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    # Call train and validate, get attention weights from training\n",
        "\n",
        "    # Print your metrics\n",
        "\n",
        "    # Plot Attention for a single item in the batch\n",
        "    plot_attention(attention_plot[0].cpu().detach().numpy())\n",
        "\n",
        "    # Log metrics to Wandb\n",
        "\n",
        "    # Optional: Scheduler Step / Teacher Force Schedule Step\n",
        "\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        # Save your model checkpoint here"
      ],
      "metadata": {
        "id": "eDWGFIcjddz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "hgFYFaBGeBqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Load your best model Checkpoint here\n",
        "\n",
        "# TODO: Create a testing function similar to validation \n",
        "# TODO: Create a file with all predictions \n",
        "# TODO: Submit to Kaggle"
      ],
      "metadata": {
        "id": "VxEFx7ipeCqJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}