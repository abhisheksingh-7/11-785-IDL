{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJfgcHPQqntv",
        "outputId": "7b6f33af-943f-46df-cc1b-d6abdd5d7be0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Apr 24 21:08:25 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   59C    P0    29W /  70W |   8049MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # to see what GPU you have"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA9qZoIDcx-h",
        "outputId": "151064cc-ed64-4001-b7bd-e142f4f6db69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "### Levenshtein\n",
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "89873b05-04e1-41f4-cfb4-092159533197"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.5/175.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 32 (delta 14), pack-reused 1063\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 782.27 KiB | 20.59 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Total 82 (delta 0), reused 0 (delta 0), pack-reused 82        \n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14147, done.        \n",
            "remote: Counting objects: 100% (460/460), done.        \n",
            "remote: Compressing objects: 100% (320/320), done.        \n",
            "remote: Total 14147 (delta 152), reused 398 (delta 126), pack-reused 13687        \n",
            "Receiving objects: 100% (14147/14147), 5.91 MiB | 25.63 MiB/s, done.\n",
            "Resolving deltas: 100% (8032/8032), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content/ctcdecode\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install wandb --quiet\n",
        "!pip install python-Levenshtein -q\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget -q\n",
        "%cd ctcdecode\n",
        "!pip install . -q\n",
        "%cd ..\n",
        "\n",
        "!pip install torchsummaryX -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "b74e561f-a10d-4c43-cc05-ca0d4ce218d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import multiprocessing\n",
        "\n",
        "\n",
        "# imports for decoding and distance calculation\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUelfGhom1m",
        "outputId": "760e0323-bddd-43ed-9a67-302f51a8b179"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73272 sha256=dffe5df75614279fde25d41595eb41050a83ca6e1b645cb6ac81a2f9b31eb45c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/02/ef/3f8c8d86b8d5388a1d3155876837f1a1a3143ab3fc2ff1ffad\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.13\n",
            "    Uninstalling kaggle-1.5.13:\n",
            "      Successfully uninstalled kaggle-1.5.13\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"kaggle-username\",\"key\":\"kaggle-key\"}') \n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSjBwfXeoq4B",
        "outputId": "e4fe3f56-39fa-49bd-fa20-324ae77429aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-s23-hw3p2.zip to /content\n",
            "100% 15.9G/15.9G [01:49<00:00, 214MB/s]\n",
            "100% 15.9G/15.9G [01:49<00:00, 157MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c 11-785-s23-hw3p2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ruxWP60LCQA",
        "outputId": "902c63b2-34a3-4747-d6c4-5ef8758815fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11-785-s23-hw3p2.zip  ctcdecode  data  sample_data\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-f22-hw3p2.zip  ctcdecode  hw3p2\n",
        "'''\n",
        "!mkdir '/content/data'\n",
        "\n",
        "!unzip -qo '11-785-s23-hw3p2.zip' -d '/content/data'\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9v5ewZDMpYA"
      },
      "source": [
        "# Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Cp-716IMZRd"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "# This overwrites the phonetics.py file.\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\", \n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\", \n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\", \n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\", \n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\", \n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\", \n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\", \n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eN2kcxwXLLBb"
      },
      "outputs": [],
      "source": [
        "# You might want to play around with the mapping as a sanity check here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    #TODO\n",
        "    def __init__(self, data_directory): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        self.mfcc_dir = data_directory + '/mfcc/'\n",
        "        self.transcript_dir = data_directory + '/transcript/'\n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        # Making sure that we have the same no. of mfcc and transcripts\n",
        "        assert len(self.mfcc_files) == len(self.transcript_files)\n",
        "\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "        \n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        self.mfccs = []\n",
        "        self.transcripts = []\n",
        "        # Iterate through mfccs and transcripts\n",
        "        for i in range(self.length):\n",
        "          mfcc = np.load(self.mfcc_dir + self.mfcc_files[i])\n",
        "          # Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "          mfcc = (mfcc - np.mean(mfcc, axis=0, keepdims=True)) / np.std(mfcc, axis=0, keepdims=True)\n",
        "          # Load the corresponding transcript\n",
        "          # Remove [SOS] and [EOS] from the transcript \n",
        "          # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
        "          transcript  = np.load(self.transcript_dir + self.transcript_files[i])[1:-1]\n",
        "          # Map the String phoneme value to integer values for our model\n",
        "          transcript = np.array([self.PHONEMES.index(phoneme) for phoneme in transcript])\n",
        "          self.mfccs.append(mfcc)\n",
        "          self.transcripts.append(transcript)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        # Convert the MFCC array to a tensor\n",
        "        mfcc = torch.tensor(self.mfccs[ind])\n",
        "\n",
        "        # Convert the transcript to a tensor of phoneme indices\n",
        "        transcript = torch.tensor(self.transcripts[ind])\n",
        "\n",
        "        return mfcc, transcript\n",
        "\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        # batch of output phonemes\n",
        "        batch_mfcc, batch_transcript = zip(*batch)\n",
        "        \n",
        "        # pad mfcc and transcript to have the same length i.e. the length of the\n",
        "        # longest sequence in the given mini batch\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True)\n",
        "\n",
        "        # store actual (unpadded) length of each recording and transcript\n",
        "        lengths_mfcc = [len(mfcc) for mfcc in batch_mfcc]\n",
        "        lengths_transcript = [len(transcript) for transcript in batch_transcript]\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.tensor(lengths_mfcc), torch.tensor(lengths_transcript)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "#TODO\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_directory): \n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        self.mfcc_dir = data_directory + '/mfcc/'\n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "\n",
        "        self.PHONEMES = PHONEMES\n",
        "\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "        \n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        self.mfccs = []\n",
        "        # Iterate through mfccs and transcripts\n",
        "        for i in range(self.length):\n",
        "          mfcc = np.load(self.mfcc_dir + self.mfcc_files[i])\n",
        "          # Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "          mfcc = (mfcc - np.mean(mfcc, axis=0, keepdims=True)) / np.std(mfcc, axis=0, keepdims=True)\n",
        "          self.mfccs.append(mfcc)\n",
        "          \n",
        "    def __len__(self): \n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        TODO: RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        # Convert the MFCC array to a tensor\n",
        "        mfcc = torch.tensor(self.mfccs[ind])\n",
        "        return mfcc\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish. \n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features, \n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        # batch of output phonemes\n",
        "        batch_mfcc = batch\n",
        "        \n",
        "        # pad mfcc to have the same length i.e. the length of the\n",
        "        # longest sequence in the given mini batch\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)\n",
        "\n",
        "        # store actual (unpadded) length of each recording\n",
        "        lengths_mfcc = [len(mfcc) for mfcc in batch_mfcc]\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "        \n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, torch.tensor(lengths_mfcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Data - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4icymeX1ImUN"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "transforms = [] # set of tranformations\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "\n",
        "root = '/content/hw3p2'\n",
        "\n",
        "config = {\n",
        "    \"beam_width\" : 2,\n",
        "    \"lr\" : 1e-3,\n",
        "    \"epochs\" : 50,\n",
        "    \"num_layers\": 2,\n",
        "    \"factor\": 0.5,\n",
        "    \"dropout\": 0.2,\n",
        "    \"patience\": 2, \n",
        "} # Feel free to add more items here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "758dc110-c746-4eb6-e76b-cc04a30793da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get me RAMMM!!!! \n",
        "import gc \n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "04e41703-acab-4288-99b4-e6e7d1f2577d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  64\n",
            "Train dataset samples = 132552, batches = 2072\n",
            "Val dataset samples = 2703, batches = 43\n",
            "Test dataset samples = 2620, batches = 41\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset(data_directory='/content/data/11-785-s23-hw3p2/train-clean-360/')\n",
        "val_data = AudioDataset(data_directory='/content/data/11-785-s23-hw3p2/dev-clean/')\n",
        "test_data = AudioDatasetTest(data_directory='/content/data/11-785-s23-hw3p2/test-clean/')\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data, \n",
        "    num_workers = 4,\n",
        "    batch_size  = BATCH_SIZE, \n",
        "    pin_memory  = True,\n",
        "    shuffle     = True,\n",
        "    collate_fn  = train_data.collate_fn\n",
        ")\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data, \n",
        "    num_workers = 2,\n",
        "    batch_size  = BATCH_SIZE,\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False,\n",
        "    collate_fn  = val_data.collate_fn\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data, \n",
        "    num_workers = 2, \n",
        "    batch_size  = BATCH_SIZE, \n",
        "    pin_memory  = True, \n",
        "    shuffle     = False,\n",
        "    collate_fn  = test_data.collate_fn\n",
        ")\n",
        "\n",
        "print(\"Batch size: \", BATCH_SIZE)\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "f24289e6-1392-4c18-f7de-ba4f1f600af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1639, 27]) torch.Size([64, 207]) torch.Size([64]) torch.Size([64])\n",
            "torch.Size([64, 2936, 27]) torch.Size([64, 265]) torch.Size([64]) torch.Size([64])\n",
            "torch.Size([64, 2001, 27]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSexxhdfMUzx"
      },
      "source": [
        "# NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLad4pChcuvX"
      },
      "source": [
        "## Basic\n",
        "\n",
        "This is a basic block for understanding, you can skip this and move to pBLSTM one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQhvHr71GJfq"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# class Network(nn.Module):\n",
        "\n",
        "#     def __init__(self, input_size, output_size):\n",
        "#         super(Network, self).__init__()\n",
        "\n",
        "#         # Adding some sort of embedding layer or feature extractor might help performance.\n",
        "#         # self.embedding = ?\n",
        "        \n",
        "#         # TODO : look up the documentation. You might need to pass some additional parameters.\n",
        "#         self.lstm = nn.LSTM(input_size=input_size, hidden_size=256, num_layers=1, bidirectional=True)\n",
        "       \n",
        "#         self.classification = nn.Sequential(\n",
        "#             nn.Linear(512, output_size)\n",
        "#             #TODO: Linear layer with in_features from the lstm module above and out_features = OUT_SIZE\n",
        "#         )\n",
        "\n",
        "#         self.logSoftmax = nn.LogSoftmax(dim=2) #TODO: Apply a log softmax here. Which dimension would apply it on ?\n",
        "\n",
        "#     def forward(self, x, lx):\n",
        "#         #TODO\n",
        "#         # The forward function takes 2 parameter inputs here. Why?\n",
        "#         # Refer to the handout for hints\n",
        "#         # x has shape (batch_size, max_seq_len, input_size)\n",
        "#         # lx is a tensor of sequence lengths, of shape (batch_size,)\n",
        "        \n",
        "#         x = x.permute(1, 0, 2)  # (max_seq_len, batch_size, input_size)\n",
        "#         packed_x = nn.utils.rnn.pack_padded_sequence(x, lx, enforce_sorted=False)\n",
        "#         packed_h, _ = self.lstm(packed_x)\n",
        "#         h, lh = nn.utils.rnn.pad_packed_sequence(packed_h)\n",
        "#         # h has shape (max_seq_len, batch_size, 512)\n",
        "\n",
        "#         # Apply linear layer and logsoftmax\n",
        "#         h = self.classification(h)\n",
        "#         h = self.logSoftmax(h)\n",
        "#         # h has shape (max_seq_len, batch_size, output_size)\n",
        "\n",
        "#         return h.permute(1, 0, 2), lh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6eh3gnMUzy"
      },
      "source": [
        "## Pyramid Bi-LSTM (pBLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd4BEX_yMUzz"
      },
      "outputs": [],
      "source": [
        "# Utils for network\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class PermuteBlock(torch.nn.Module):\n",
        "    '''\n",
        "    PyTorch conv1d expects tensors of shape (N, C, L)\n",
        "    Permuting the input aligns the feature dim with C\n",
        "    (B, T, 27) -> (B, 27, T)\n",
        "    '''\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmdyXI6KMUzz"
      },
      "outputs": [],
      "source": [
        "class pBLSTM(torch.nn.Module):\n",
        "\n",
        "    '''\n",
        "    Pyramidal BiLSTM\n",
        "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
        "\n",
        "    At each step,\n",
        "    1. Pad your input if it is packed (Unpack it)\n",
        "    2. Reduce the input length dimension by concatenating feature dimension\n",
        "        (Tip: Write down the shapes and understand)\n",
        "        (i) How should  you deal with odd/even length input? \n",
        "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
        "    3. Pack your input\n",
        "    4. Pass it into LSTM layer\n",
        "\n",
        "    To make our implementation modular, we pass 1 layer at a time.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        # Initialize a single layer bidirectional LSTM with the given input_size and hidden_size\n",
        "        self.blstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=config['num_layers'], batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
        "\n",
        "        # Pad Packed Sequence \n",
        "        if isinstance(x_packed, torch.nn.utils.rnn.PackedSequence):\n",
        "          # pad a packed sequence after computation from the RNN\n",
        "          # seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch first=True)\n",
        "          x, x_lens = pad_packed_sequence(x_packed, batch_first=True)\n",
        "        else:\n",
        "          x = x_packed\n",
        "          x_lens = None\n",
        "\n",
        "        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
        "        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
        "        x, x_lens = self.trunc_reshape(x, x_lens)\n",
        "        \n",
        "        # Pack Padded Sequence. What output(s) would you get?\n",
        "        if x_lens is not None:\n",
        "          # pack a padded sequence before passing it through an RNN\n",
        "          # packed = pack_padded_sequence(recordings, length_of_each_recording, batch_first=True, enforce_sorted=False)\n",
        "          x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "        else:\n",
        "          x_packed = x\n",
        "\n",
        "        # Pass the sequence through bLSTM\n",
        "        output, (h_n, c_n) = self.blstm(x_packed)\n",
        "\n",
        "        # What do you return?\n",
        "        return output\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens): \n",
        "        batch_size, length, features = x.shape\n",
        "        # TODO: If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
        "        '''\n",
        "        If the length of the original input sequence is odd, then it is either padded out by appending\n",
        "        an extra vector of zeros at the end, before reshaping, or trimmed to an even length by deleting\n",
        "        the final vector. \n",
        "        '''\n",
        "        if (length % 2) != 0:\n",
        "          # trim to an even length by deleting the final vector and decrementing length\n",
        "          x = x[:, :-1, :]\n",
        "          x_lens[-1] -= 1\n",
        "        \n",
        "        downsampled_length = length // 2\n",
        "        # Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
        "        x = torch.reshape(x, shape=(batch_size, downsampled_length, 2 * features))\n",
        "        # Reduce lengths by the same downsampling factor = 2\n",
        "        x_lens = torch.clamp(x_lens, max=downsampled_length, out=None)\n",
        "        return x, x_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3ZQ75OcMUz0"
      },
      "source": [
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aBGKOKNLdh0e"
      },
      "outputs": [],
      "source": [
        "class LockedDropout(nn.Module):\n",
        "  '''\n",
        "  Locked dropout is a variation of dropout that is specifically designed for use in \n",
        "  the parallel bidirectional LSTM (pBLSTM) architecture. It uses the same dropout mask \n",
        "  for both the forward and backward layers.  This ensures that the same set of neurons \n",
        "  are dropped out at each time step for both layers, maintaining consistency across time.\n",
        "\n",
        "  From https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py\n",
        "  '''\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "  def forward(self, x, dropout=config['dropout']):\n",
        "    if not self.training or not dropout:\n",
        "      return x\n",
        "    x, x_lens = pad_packed_sequence(x, batch_first=True)\n",
        "    m = x.new(x.shape[0],1,x.shape[2]).bernoulli_(1-dropout)\n",
        "    mask = Variable(m, requires_grad=False) / (1 - dropout)\n",
        "    mask = mask.expand_as(x)\n",
        "    x = mask * x\n",
        "    # pack a padded sequence using before passing it through an RNN\n",
        "    x_packed = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "    return x_packed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEzw5_xmMUz0"
      },
      "outputs": [],
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    '''\n",
        "    The Encoder takes utterances as inputs and returns latent feature representations\n",
        "    '''\n",
        "    def __init__(self, input_size, embed_size, hidden_size):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        # You can use CNNs as Embedding layer to extract features. \n",
        "        # Keep in mind the Input dimensions and expected dimension of Pytorch CNN.\n",
        "        self.embedding = torch.nn.Sequential(\n",
        "            PermuteBlock(), # transpose to (N, C, L) for Conv1d\n",
        "            nn.Conv1d(input_size, embed_size, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm1d(embed_size),\n",
        "            PermuteBlock(), # transpose back for RNN\n",
        "        )\n",
        "        # How many pBLSTMs are required?\n",
        "        # Fill this up with pBLSTMs - What should the input_size be? \n",
        "        # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
        "        self.pBLSTMs = torch.nn.Sequential(\n",
        "            pBLSTM(input_size=2*embed_size, hidden_size=hidden_size),\n",
        "            LockedDropout(),\n",
        "            pBLSTM(input_size=4*hidden_size, hidden_size=hidden_size),\n",
        "            LockedDropout(),\n",
        "        )          \n",
        "    def forward(self, x, x_lens):\n",
        "        # Where are x and x_lens coming from? The dataloader\n",
        "        \n",
        "        # Call the embedding layer\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # Pack padded sequence\n",
        "        x = pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Pass Sequence through the pyramidal Bi-LSTM layer\n",
        "        x = self.pBLSTMs(x)\n",
        "\n",
        "        # Pad packed sequence\n",
        "        encoder_outputs, encoder_lens = pad_packed_sequence(x, batch_first=True)\n",
        "\n",
        "        # Return encoder outputs and encoder sequence lengths\n",
        "        return encoder_outputs, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg82HXa3MUz1"
      },
      "source": [
        "# Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQIRxdNTMUz1"
      },
      "outputs": [],
      "source": [
        "class Decoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, output_size= 41):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mlp = torch.nn.Sequential(\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(2 * embed_size), PermuteBlock(),\n",
        "            #Use Permute Block before and after BatchNorm1d() to match the size\n",
        "            torch.nn.Linear(2*embed_size, 2048),\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(2048), PermuteBlock(),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Linear(2048, 2048),\n",
        "            PermuteBlock(), torch.nn.BatchNorm1d(2048), PermuteBlock(),\n",
        "            torch.nn.GELU(),\n",
        "            torch.nn.Dropout(config['dropout']),\n",
        "            torch.nn.Linear(2048, output_size)\n",
        "        )\n",
        "        \n",
        "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        # call your MLP\n",
        "        out = self.mlp(encoder_out)\n",
        "        # Think what should be the final output of the decoder for the classification \n",
        "        out = self.softmax(out)\n",
        "\n",
        "        return out "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmHf6pFiMUz1"
      },
      "outputs": [],
      "source": [
        "class ASRModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, embed_size, hidden_size, output_size= len(PHONEMES)):\n",
        "        super().__init__()\n",
        "\n",
        "        self.augmentations = torch.nn.Sequential(\n",
        "            # Add Time Masking/ Frequency Masking\n",
        "            # Hint: See how to use PermuteBlock() function defined above\n",
        "            PermuteBlock(),\n",
        "            tat.FrequencyMasking(freq_mask_param=10),\n",
        "            tat.TimeMasking(time_mask_param=40),\n",
        "            PermuteBlock(),\n",
        "        )\n",
        "        # Initialize Encoder\n",
        "        self.encoder = Encoder(input_size=input_size, embed_size = embed_size, hidden_size=hidden_size)\n",
        "        # Initialize Decoder\n",
        "        self.decoder = Decoder(embed_size=hidden_size, output_size=output_size) \n",
        "        \n",
        "    \n",
        "    def forward(self, x, lengths_x):\n",
        "        if self.training:\n",
        "            x = self.augmentations(x)\n",
        "\n",
        "        encoder_out, encoder_lens   = self.encoder(x, lengths_x)\n",
        "        decoder_out                 = self.decoder(encoder_out)\n",
        "\n",
        "        return decoder_out, encoder_lens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUThsowyQdN7"
      },
      "source": [
        "## INIT\n",
        "(If trying out the basic Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CGoiXd70tb5z"
      },
      "outputs": [],
      "source": [
        "# torch.cuda.empty_cache()\n",
        "\n",
        "# model = Network(\n",
        "#     input_size  = 27,\n",
        "#     output_size = len(PHONEMES)).to(device)\n",
        "# summary(model, x.to(device), lx) # x and lx come from the sanity check above :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EV7DMPDoMUz2"
      },
      "source": [
        "## INIT ASR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oaaDsnnLMUz2",
        "outputId": "f902670e-3136-4530-deda-ee8a18f04010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASRModel(\n",
            "  (augmentations): Sequential(\n",
            "    (0): PermuteBlock()\n",
            "    (1): FrequencyMasking()\n",
            "    (2): TimeMasking()\n",
            "    (3): PermuteBlock()\n",
            "  )\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): Conv1d(27, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
            "      (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): PermuteBlock()\n",
            "    )\n",
            "    (pBLSTMs): Sequential(\n",
            "      (0): pBLSTM(\n",
            "        (blstm): LSTM(512, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "      )\n",
            "      (1): LockedDropout()\n",
            "      (2): pBLSTM(\n",
            "        (blstm): LSTM(1024, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "      )\n",
            "      (3): LockedDropout()\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (mlp): Sequential(\n",
            "      (0): PermuteBlock()\n",
            "      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (2): PermuteBlock()\n",
            "      (3): Linear(in_features=512, out_features=2048, bias=True)\n",
            "      (4): PermuteBlock()\n",
            "      (5): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (6): PermuteBlock()\n",
            "      (7): GELU(approximate='none')\n",
            "      (8): Linear(in_features=2048, out_features=2048, bias=True)\n",
            "      (9): PermuteBlock()\n",
            "      (10): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (11): PermuteBlock()\n",
            "      (12): GELU(approximate='none')\n",
            "      (13): Dropout(p=0.2, inplace=False)\n",
            "      (14): Linear(in_features=2048, out_features=41, bias=True)\n",
            "    )\n",
            "    (softmax): LogSoftmax(dim=2)\n",
            "  )\n",
            ")\n",
            "========================================================================================\n",
            "                                    Kernel Shape     Output Shape     Params  \\\n",
            "Layer                                                                          \n",
            "0_augmentations.PermuteBlock_0                 -   [64, 27, 2001]          -   \n",
            "1_augmentations.FrequencyMasking_1             -   [64, 27, 2001]          -   \n",
            "2_augmentations.TimeMasking_2                  -   [64, 27, 2001]          -   \n",
            "3_augmentations.PermuteBlock_3                 -   [64, 2001, 27]          -   \n",
            "4_encoder.embedding.PermuteBlock_0             -   [64, 27, 2001]          -   \n",
            "5_encoder.embedding.Conv1d_1        [27, 256, 3]  [64, 256, 2001]    20.736k   \n",
            "6_encoder.embedding.BatchNorm1d_2          [256]  [64, 256, 2001]      512.0   \n",
            "7_encoder.embedding.PermuteBlock_3             -  [64, 2001, 256]          -   \n",
            "8_encoder.pBLSTMs.0.LSTM_blstm                 -     [40989, 512]   3.15392M   \n",
            "9_encoder.pBLSTMs.LockedDropout_1              -     [40989, 512]          -   \n",
            "10_encoder.pBLSTMs.2.LSTM_blstm                -     [27909, 512]  4.202496M   \n",
            "11_encoder.pBLSTMs.LockedDropout_3             -     [27909, 512]          -   \n",
            "12_decoder.mlp.PermuteBlock_0                  -   [64, 512, 500]          -   \n",
            "13_decoder.mlp.BatchNorm1d_1               [512]   [64, 512, 500]     1.024k   \n",
            "14_decoder.mlp.PermuteBlock_2                  -   [64, 500, 512]          -   \n",
            "15_decoder.mlp.Linear_3              [512, 2048]  [64, 500, 2048]  1.050624M   \n",
            "16_decoder.mlp.PermuteBlock_4                  -  [64, 2048, 500]          -   \n",
            "17_decoder.mlp.BatchNorm1d_5              [2048]  [64, 2048, 500]     4.096k   \n",
            "18_decoder.mlp.PermuteBlock_6                  -  [64, 500, 2048]          -   \n",
            "19_decoder.mlp.GELU_7                          -  [64, 500, 2048]          -   \n",
            "20_decoder.mlp.Linear_8             [2048, 2048]  [64, 500, 2048]  4.196352M   \n",
            "21_decoder.mlp.PermuteBlock_9                  -  [64, 2048, 500]          -   \n",
            "22_decoder.mlp.BatchNorm1d_10             [2048]  [64, 2048, 500]     4.096k   \n",
            "23_decoder.mlp.PermuteBlock_11                 -  [64, 500, 2048]          -   \n",
            "24_decoder.mlp.GELU_12                         -  [64, 500, 2048]          -   \n",
            "25_decoder.mlp.Dropout_13                      -  [64, 500, 2048]          -   \n",
            "26_decoder.mlp.Linear_14              [2048, 41]    [64, 500, 41]    84.009k   \n",
            "27_decoder.LogSoftmax_softmax                  -    [64, 500, 41]          -   \n",
            "\n",
            "                                     Mult-Adds  \n",
            "Layer                                           \n",
            "0_augmentations.PermuteBlock_0               -  \n",
            "1_augmentations.FrequencyMasking_1           -  \n",
            "2_augmentations.TimeMasking_2                -  \n",
            "3_augmentations.PermuteBlock_3               -  \n",
            "4_encoder.embedding.PermuteBlock_0           -  \n",
            "5_encoder.embedding.Conv1d_1        41.492736M  \n",
            "6_encoder.embedding.BatchNorm1d_2        256.0  \n",
            "7_encoder.embedding.PermuteBlock_3           -  \n",
            "8_encoder.pBLSTMs.0.LSTM_blstm       3.145728M  \n",
            "9_encoder.pBLSTMs.LockedDropout_1            -  \n",
            "10_encoder.pBLSTMs.2.LSTM_blstm      4.194304M  \n",
            "11_encoder.pBLSTMs.LockedDropout_3           -  \n",
            "12_decoder.mlp.PermuteBlock_0                -  \n",
            "13_decoder.mlp.BatchNorm1d_1             512.0  \n",
            "14_decoder.mlp.PermuteBlock_2                -  \n",
            "15_decoder.mlp.Linear_3              1.048576M  \n",
            "16_decoder.mlp.PermuteBlock_4                -  \n",
            "17_decoder.mlp.BatchNorm1d_5            2.048k  \n",
            "18_decoder.mlp.PermuteBlock_6                -  \n",
            "19_decoder.mlp.GELU_7                        -  \n",
            "20_decoder.mlp.Linear_8              4.194304M  \n",
            "21_decoder.mlp.PermuteBlock_9                -  \n",
            "22_decoder.mlp.BatchNorm1d_10           2.048k  \n",
            "23_decoder.mlp.PermuteBlock_11               -  \n",
            "24_decoder.mlp.GELU_12                       -  \n",
            "25_decoder.mlp.Dropout_13                    -  \n",
            "26_decoder.mlp.Linear_14               83.968k  \n",
            "27_decoder.LogSoftmax_softmax                -  \n",
            "----------------------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params          12.717865M\n",
            "Trainable params      12.717865M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds              54.16448M\n",
            "========================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ee7767d4-59c1-4f91-aaae-3bd4a7f1fe8f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_augmentations.PermuteBlock_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 27, 2001]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_augmentations.FrequencyMasking_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 27, 2001]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_augmentations.TimeMasking_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 27, 2001]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_augmentations.PermuteBlock_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 2001, 27]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_encoder.embedding.PermuteBlock_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 27, 2001]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_encoder.embedding.Conv1d_1</th>\n",
              "      <td>[27, 256, 3]</td>\n",
              "      <td>[64, 256, 2001]</td>\n",
              "      <td>20736.0</td>\n",
              "      <td>41492736.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_encoder.embedding.BatchNorm1d_2</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[64, 256, 2001]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_encoder.embedding.PermuteBlock_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 2001, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_encoder.pBLSTMs.0.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[40989, 512]</td>\n",
              "      <td>3153920.0</td>\n",
              "      <td>3145728.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_encoder.pBLSTMs.LockedDropout_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[40989, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_encoder.pBLSTMs.2.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[27909, 512]</td>\n",
              "      <td>4202496.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_encoder.pBLSTMs.LockedDropout_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[27909, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_decoder.mlp.PermuteBlock_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 512, 500]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_decoder.mlp.BatchNorm1d_1</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[64, 512, 500]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_decoder.mlp.PermuteBlock_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 500, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_decoder.mlp.Linear_3</th>\n",
              "      <td>[512, 2048]</td>\n",
              "      <td>[64, 500, 2048]</td>\n",
              "      <td>1050624.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_decoder.mlp.PermuteBlock_4</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 2048, 500]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_decoder.mlp.BatchNorm1d_5</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[64, 2048, 500]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_decoder.mlp.PermuteBlock_6</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 500, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_decoder.mlp.GELU_7</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 500, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_decoder.mlp.Linear_8</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[64, 500, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_decoder.mlp.PermuteBlock_9</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 2048, 500]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_decoder.mlp.BatchNorm1d_10</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[64, 2048, 500]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_decoder.mlp.PermuteBlock_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 500, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_decoder.mlp.GELU_12</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 500, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_decoder.mlp.Dropout_13</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 500, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_decoder.mlp.Linear_14</th>\n",
              "      <td>[2048, 41]</td>\n",
              "      <td>[64, 500, 41]</td>\n",
              "      <td>84009.0</td>\n",
              "      <td>83968.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_decoder.LogSoftmax_softmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 500, 41]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ee7767d4-59c1-4f91-aaae-3bd4a7f1fe8f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ee7767d4-59c1-4f91-aaae-3bd4a7f1fe8f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ee7767d4-59c1-4f91-aaae-3bd4a7f1fe8f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                    Kernel Shape     Output Shape     Params  \\\n",
              "Layer                                                                          \n",
              "0_augmentations.PermuteBlock_0                 -   [64, 27, 2001]        NaN   \n",
              "1_augmentations.FrequencyMasking_1             -   [64, 27, 2001]        NaN   \n",
              "2_augmentations.TimeMasking_2                  -   [64, 27, 2001]        NaN   \n",
              "3_augmentations.PermuteBlock_3                 -   [64, 2001, 27]        NaN   \n",
              "4_encoder.embedding.PermuteBlock_0             -   [64, 27, 2001]        NaN   \n",
              "5_encoder.embedding.Conv1d_1        [27, 256, 3]  [64, 256, 2001]    20736.0   \n",
              "6_encoder.embedding.BatchNorm1d_2          [256]  [64, 256, 2001]      512.0   \n",
              "7_encoder.embedding.PermuteBlock_3             -  [64, 2001, 256]        NaN   \n",
              "8_encoder.pBLSTMs.0.LSTM_blstm                 -     [40989, 512]  3153920.0   \n",
              "9_encoder.pBLSTMs.LockedDropout_1              -     [40989, 512]        NaN   \n",
              "10_encoder.pBLSTMs.2.LSTM_blstm                -     [27909, 512]  4202496.0   \n",
              "11_encoder.pBLSTMs.LockedDropout_3             -     [27909, 512]        NaN   \n",
              "12_decoder.mlp.PermuteBlock_0                  -   [64, 512, 500]        NaN   \n",
              "13_decoder.mlp.BatchNorm1d_1               [512]   [64, 512, 500]     1024.0   \n",
              "14_decoder.mlp.PermuteBlock_2                  -   [64, 500, 512]        NaN   \n",
              "15_decoder.mlp.Linear_3              [512, 2048]  [64, 500, 2048]  1050624.0   \n",
              "16_decoder.mlp.PermuteBlock_4                  -  [64, 2048, 500]        NaN   \n",
              "17_decoder.mlp.BatchNorm1d_5              [2048]  [64, 2048, 500]     4096.0   \n",
              "18_decoder.mlp.PermuteBlock_6                  -  [64, 500, 2048]        NaN   \n",
              "19_decoder.mlp.GELU_7                          -  [64, 500, 2048]        NaN   \n",
              "20_decoder.mlp.Linear_8             [2048, 2048]  [64, 500, 2048]  4196352.0   \n",
              "21_decoder.mlp.PermuteBlock_9                  -  [64, 2048, 500]        NaN   \n",
              "22_decoder.mlp.BatchNorm1d_10             [2048]  [64, 2048, 500]     4096.0   \n",
              "23_decoder.mlp.PermuteBlock_11                 -  [64, 500, 2048]        NaN   \n",
              "24_decoder.mlp.GELU_12                         -  [64, 500, 2048]        NaN   \n",
              "25_decoder.mlp.Dropout_13                      -  [64, 500, 2048]        NaN   \n",
              "26_decoder.mlp.Linear_14              [2048, 41]    [64, 500, 41]    84009.0   \n",
              "27_decoder.LogSoftmax_softmax                  -    [64, 500, 41]        NaN   \n",
              "\n",
              "                                     Mult-Adds  \n",
              "Layer                                           \n",
              "0_augmentations.PermuteBlock_0             NaN  \n",
              "1_augmentations.FrequencyMasking_1         NaN  \n",
              "2_augmentations.TimeMasking_2              NaN  \n",
              "3_augmentations.PermuteBlock_3             NaN  \n",
              "4_encoder.embedding.PermuteBlock_0         NaN  \n",
              "5_encoder.embedding.Conv1d_1        41492736.0  \n",
              "6_encoder.embedding.BatchNorm1d_2        256.0  \n",
              "7_encoder.embedding.PermuteBlock_3         NaN  \n",
              "8_encoder.pBLSTMs.0.LSTM_blstm       3145728.0  \n",
              "9_encoder.pBLSTMs.LockedDropout_1          NaN  \n",
              "10_encoder.pBLSTMs.2.LSTM_blstm      4194304.0  \n",
              "11_encoder.pBLSTMs.LockedDropout_3         NaN  \n",
              "12_decoder.mlp.PermuteBlock_0              NaN  \n",
              "13_decoder.mlp.BatchNorm1d_1             512.0  \n",
              "14_decoder.mlp.PermuteBlock_2              NaN  \n",
              "15_decoder.mlp.Linear_3              1048576.0  \n",
              "16_decoder.mlp.PermuteBlock_4              NaN  \n",
              "17_decoder.mlp.BatchNorm1d_5            2048.0  \n",
              "18_decoder.mlp.PermuteBlock_6              NaN  \n",
              "19_decoder.mlp.GELU_7                      NaN  \n",
              "20_decoder.mlp.Linear_8              4194304.0  \n",
              "21_decoder.mlp.PermuteBlock_9              NaN  \n",
              "22_decoder.mlp.BatchNorm1d_10           2048.0  \n",
              "23_decoder.mlp.PermuteBlock_11             NaN  \n",
              "24_decoder.mlp.GELU_12                     NaN  \n",
              "25_decoder.mlp.Dropout_13                  NaN  \n",
              "26_decoder.mlp.Linear_14               83968.0  \n",
              "27_decoder.LogSoftmax_softmax              NaN  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = ASRModel(\n",
        "    input_size  = 27,\n",
        "    embed_size  = 256,\n",
        "    hidden_size = 256,\n",
        "    output_size = len(PHONEMES)\n",
        ").to(device)\n",
        "print(model)\n",
        "torchsummaryX.summary(model, x.to(device), lx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "# Define CTC loss as the criterion. How would the losses be reduced?\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "criterion = torch.nn.CTCLoss(blank=0, reduction='mean') \n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=config['lr'])\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "decoder = CTCBeamDecoder(\n",
        "    LABELS,\n",
        "    alpha=0,\n",
        "    beta=0,\n",
        "    cutoff_top_n=len(LABELS),\n",
        "    cutoff_prob=1.0,\n",
        "    beam_width=config['beam_width'],\n",
        "    num_processes=multiprocessing.cpu_count(),\n",
        "    blank_id=0,\n",
        "    log_probs_input=True\n",
        ")\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=config['patience'], factor=config['factor'])\n",
        "\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "## Decode Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n",
        "    \n",
        "    # look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(output, seq_lens= output_lens) #lengths - list of lengths\n",
        "\n",
        "    pred_strings                    = []\n",
        "    \n",
        "    for i in range(output_lens.shape[0]):\n",
        "        # Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n",
        "        prediction = \"\"\n",
        "        for j in range(out_lens[i, 0]):\n",
        "            prediction += PHONEME_MAP[beam_results[i, 0, j]]\n",
        "        pred_strings.append(prediction)\n",
        "    \n",
        "    return pred_strings\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n",
        "    \n",
        "    dist            = 0\n",
        "    batch_size      = label.shape[0]\n",
        "\n",
        "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "    label_strings = [\"\".join(PHONEME_MAP[i] for i in label[j, :label_lens[j]].tolist()) for j in range(batch_size)]\n",
        "    for i in range(batch_size):\n",
        "        dist += Levenshtein.distance(pred_strings[i], label_strings[i])\n",
        "    dist /= batch_size # TODO: Uncomment this, but think about why we are doing this\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnTLL-5gMBrY",
        "outputId": "7b7bc4e4-179c-4745-88e5-486735a322c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 734, 41])\n",
            "201.484375\n",
            "torch.Size([734, 64, 41]) torch.Size([64, 265])\n",
            "tensor(32.2683, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0): #TODO: valid_loader\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS)) #TODO: valid_decoder\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "\n",
        "    \n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0ht9r03Elmq",
        "outputId": "64082028-0ea2-4ca2-c997-4f2802519f91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"<replace with your API key here>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "F3GJBO3TE2oT",
        "outputId": "828302aa-eb98-4ff9-f8ab-f12a92d11c6c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230425_034404-ntwjqxn8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/deeper_learners/hw3p2-ablations/runs/ntwjqxn8' target=\"_blank\">HW3P2-gcp-ctd</a></strong> to <a href='https://wandb.ai/deeper_learners/hw3p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/deeper_learners/hw3p2-ablations' target=\"_blank\">https://wandb.ai/deeper_learners/hw3p2-ablations</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/deeper_learners/hw3p2-ablations/runs/ntwjqxn8' target=\"_blank\">https://wandb.ai/deeper_learners/hw3p2-ablations/runs/ntwjqxn8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"HW3P2-gcp-ctd\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account \n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "    \n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train') \n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():     \n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16. \n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss \n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "    \n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "    \n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGrEFE38MUz5",
        "outputId": "070329da-919f-4dc9-cffc-57ad3483a21e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 734, 41])\n",
            "201.484375\n",
            "torch.Size([734, 64, 41]) torch.Size([64, 265])\n",
            "tensor(32.2683, device='cuda:0', grad_fn=<MeanBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    print(calculate_levenshtein(h, y, lx, ly, decoder, LABELS))\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "\n",
        "    \n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "### Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "def save_model(model, file, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1], \n",
        "         'epoch'                    : epoch}, \n",
        "         path + file +'.pth'\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        \n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "outputs": [],
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "epoch_model_path = \"/content/epoch_models\" #TODO set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
        "best_model_path = \"/content/best_models\" #TODO set best model path "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model, optimizer, scheduler, epoch, metric = load_model('/content/best_models/hw3p2_model_checkpoint.pth', model, 'valid_dist', optimizer=optimizer, scheduler=scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JR43E28rM9Ak",
        "outputId": "8dabd7c9-af8d-4df4-9f4e-3ecb6647fb6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1800\t Learning Rate 0.0010000\n",
            "\tVal Dist 3.2712%\t Val Loss 0.1607\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 2/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1747\t Learning Rate 0.0010000\n",
            "\tVal Dist 3.1657%\t Val Loss 0.1568\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 3/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1716\t Learning Rate 0.0010000\n",
            "\tVal Dist 3.1722%\t Val Loss 0.1575\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 4/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1700\t Learning Rate 0.0010000\n",
            "\tVal Dist 3.1513%\t Val Loss 0.1568\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 5/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1621\t Learning Rate 0.0010000\n",
            "\tVal Dist 3.0755%\t Val Loss 0.1538\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 6/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1612\t Learning Rate 0.0010000\n",
            "\tVal Dist 3.0661%\t Val Loss 0.1541\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 7/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1604\t Learning Rate 0.0010000\n",
            "\tVal Dist 3.0785%\t Val Loss 0.1532\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 8/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1571\t Learning Rate 0.0010000\n",
            "\tVal Dist 2.9875%\t Val Loss 0.1506\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 9/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1532\t Learning Rate 0.0010000\n",
            "\tVal Dist 2.9532%\t Val Loss 0.1484\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 10/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1517\t Learning Rate 0.0010000\n",
            "\tVal Dist 2.9618%\t Val Loss 0.1490\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 11/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1476\t Learning Rate 0.0010000\n",
            "\tVal Dist 2.9098%\t Val Loss 0.1456\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 12/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1445\t Learning Rate 0.0010000\n",
            "\tVal Dist 2.9320%\t Val Loss 0.1479\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 13/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1455\t Learning Rate 0.0010000\n",
            "\tVal Dist 2.9162%\t Val Loss 0.1487\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 14/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1414\t Learning Rate 0.0010000\n",
            "\tVal Dist 2.8656%\t Val Loss 0.1475\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 15/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1375\t Learning Rate 0.0010000\n",
            "\tVal Dist 2.9081%\t Val Loss 0.1466\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 16/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1391\t Learning Rate 0.0010000\n",
            "\tVal Dist 2.9088%\t Val Loss 0.1468\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 17/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1372\t Learning Rate 0.0010000\n",
            "\tVal Dist 2.8851%\t Val Loss 0.1502\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 18/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1163\t Learning Rate 0.0005000\n",
            "\tVal Dist 2.6001%\t Val Loss 0.1388\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 19/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1115\t Learning Rate 0.0005000\n",
            "\tVal Dist 2.6438%\t Val Loss 0.1406\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 20/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1073\t Learning Rate 0.0005000\n",
            "\tVal Dist 2.6045%\t Val Loss 0.1398\n",
            "Saved epoch model\n",
            "\n",
            "Epoch: 21/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Loss 0.1058\t Learning Rate 0.0005000\n",
            "\tVal Dist 2.5937%\t Val Loss 0.1402\n",
            "Saved epoch model\n",
            "Saved best model\n",
            "\n",
            "Epoch: 22/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train:  40%|███▉      | 823/2072 [09:50<13:00,  1.60it/s, loss=0.1005, lr=0.000500]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-7efd2f0d3a10>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcurr_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dist\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphoneme_map\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mLABELS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-b404a22a5f25>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# Another couple things you need for FP16.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for loss.backward()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is a replacement for optimizer.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# This is something added just for FP16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Please complete the training loop\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "    \n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])#TODO\n",
        "\n",
        "    train_loss              = train_model(model, train_loader, criterion, optimizer) #TODO\n",
        "    valid_loss, valid_dist  = validate_model(model, val_loader, decoder, phoneme_map= LABELS) #TODO\n",
        "    scheduler.step(valid_dist)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,  \n",
        "        'valid_dist': valid_dist, \n",
        "        'valid_loss': valid_loss, \n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "    \n",
        "    save_model(model, '/hw3p2_model_checkpoint_'+ str(epoch), optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    wandb.save(epoch_model_path)\n",
        "    print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, '/hw3p2_model_checkpoint', optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332,
          "referenced_widgets": [
            "0cb2f07411354324a8453e036e9e6381",
            "58c1fb614c694f70b0fb699187911f3b",
            "5131854c8cbc42c8afe44d50ae0700a6",
            "75bd759b8b10480aaf278f43202894b3",
            "983121c6636841bc933ddab7d8c4415b",
            "94150d4927e44770a6aa0e3095683764",
            "6d99878f563145cda52d8b3f49112190",
            "34b56fb0bb7c4a0bb9aa38f631023f96"
          ]
        },
        "id": "pAZrqmFSfC8s",
        "outputId": "4ced5105-5d9e-46c1-cbfc-6c721ce7bdcb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cb2f07411354324a8453e036e9e6381",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>█████████████████▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▂▂▁▁</td></tr><tr><td>valid_dist</td><td>█▇▇▇▆▆▆▅▅▅▄▄▄▄▄▄▄▁▂▁▁</td></tr><tr><td>valid_loss</td><td>█▇▇▇▆▆▆▅▄▄▃▄▄▄▃▄▅▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.0005</td></tr><tr><td>train_loss</td><td>0.10578</td></tr><tr><td>valid_dist</td><td>2.59373</td></tr><tr><td>valid_loss</td><td>0.14017</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">HW3P2-gcp-ctd</strong> at: <a href='https://wandb.ai/deeper_learners/hw3p2-ablations/runs/ntwjqxn8' target=\"_blank\">https://wandb.ai/deeper_learners/hw3p2-ablations/runs/ntwjqxn8</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230425_034404-ntwjqxn8/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2moYJhTWsOG-",
        "outputId": "324e69b5-74e1-4fa9-f6e2-57d50e515db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/41 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▏         | 1/41 [00:02<01:27,  2.18s/it]\u001b[A\n",
            "  5%|▍         | 2/41 [00:03<01:02,  1.61s/it]\u001b[A\n",
            "  7%|▋         | 3/41 [00:04<00:54,  1.44s/it]\u001b[A\n",
            " 10%|▉         | 4/41 [00:05<00:46,  1.26s/it]\u001b[A\n",
            " 12%|█▏        | 5/41 [00:06<00:43,  1.21s/it]\u001b[A\n",
            " 15%|█▍        | 6/41 [00:07<00:37,  1.09s/it]\u001b[A\n",
            " 17%|█▋        | 7/41 [00:08<00:30,  1.12it/s]\u001b[A\n",
            " 20%|█▉        | 8/41 [00:09<00:33,  1.01s/it]\u001b[A\n",
            " 22%|██▏       | 9/41 [00:10<00:33,  1.06s/it]\u001b[A\n",
            " 24%|██▍       | 10/41 [00:11<00:34,  1.11s/it]\u001b[A\n",
            " 27%|██▋       | 11/41 [00:12<00:28,  1.05it/s]\u001b[A\n",
            " 29%|██▉       | 12/41 [00:13<00:27,  1.06it/s]\u001b[A\n",
            " 32%|███▏      | 13/41 [00:13<00:24,  1.13it/s]\u001b[A\n",
            " 34%|███▍      | 14/41 [00:15<00:25,  1.04it/s]\u001b[A\n",
            " 37%|███▋      | 15/41 [00:16<00:27,  1.05s/it]\u001b[A\n",
            " 39%|███▉      | 16/41 [00:17<00:27,  1.12s/it]\u001b[A\n",
            " 41%|████▏     | 17/41 [00:18<00:28,  1.17s/it]\u001b[A\n",
            " 44%|████▍     | 18/41 [00:20<00:26,  1.15s/it]\u001b[A\n",
            " 46%|████▋     | 19/41 [00:20<00:20,  1.06it/s]\u001b[A\n",
            " 49%|████▉     | 20/41 [00:21<00:21,  1.01s/it]\u001b[A\n",
            " 51%|█████     | 21/41 [00:22<00:21,  1.06s/it]\u001b[A\n",
            " 54%|█████▎    | 22/41 [00:24<00:20,  1.10s/it]\u001b[A\n",
            " 56%|█████▌    | 23/41 [00:25<00:20,  1.12s/it]\u001b[A\n",
            " 59%|█████▊    | 24/41 [00:25<00:15,  1.08it/s]\u001b[A\n",
            " 61%|██████    | 25/41 [00:26<00:16,  1.03s/it]\u001b[A\n",
            " 63%|██████▎   | 26/41 [00:27<00:15,  1.00s/it]\u001b[A\n",
            " 66%|██████▌   | 27/41 [00:28<00:12,  1.11it/s]\u001b[A\n",
            " 68%|██████▊   | 28/41 [00:29<00:11,  1.18it/s]\u001b[A\n",
            " 71%|███████   | 29/41 [00:30<00:10,  1.10it/s]\u001b[A\n",
            " 73%|███████▎  | 30/41 [00:30<00:09,  1.21it/s]\u001b[A\n",
            " 76%|███████▌  | 31/41 [00:31<00:08,  1.14it/s]\u001b[A\n",
            " 78%|███████▊  | 32/41 [00:33<00:08,  1.04it/s]\u001b[A\n",
            " 80%|████████  | 33/41 [00:34<00:07,  1.02it/s]\u001b[A\n",
            " 83%|████████▎ | 34/41 [00:35<00:06,  1.02it/s]\u001b[A\n",
            " 85%|████████▌ | 35/41 [00:36<00:05,  1.04it/s]\u001b[A\n",
            " 88%|████████▊ | 36/41 [00:37<00:05,  1.10s/it]\u001b[A\n",
            " 90%|█████████ | 37/41 [00:38<00:04,  1.13s/it]\u001b[A\n",
            " 93%|█████████▎| 38/41 [00:39<00:03,  1.03s/it]\u001b[A\n",
            " 95%|█████████▌| 39/41 [00:40<00:01,  1.02it/s]\u001b[A\n",
            " 98%|█████████▊| 40/41 [00:41<00:00,  1.02it/s]\u001b[A\n",
            "100%|██████████| 41/41 [00:43<00:00,  1.05s/it]\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "TEST_BEAM_WIDTH = 10\n",
        "\n",
        "test_decoder = CTCBeamDecoder(\n",
        "    PHONEMES,\n",
        "    beam_width=TEST_BEAM_WIDTH,\n",
        "    blank_id=0,\n",
        "    log_probs_input=True\n",
        ")\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "\n",
        "    prediction_string= decode_prediction(h, lh, test_decoder) #  call decode_prediction \n",
        "    # save the output in results array.\n",
        "    results.append(prediction_string)\n",
        "    \n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "results = [element for i in results for element in i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d70dvu_lsMlv",
        "outputId": "72949e6a-a692-4890-c1ba-2aedaeb12d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.13 / client 1.5.8)\n",
            "100% 210k/210k [00:00<00:00, 445kB/s]\n",
            "Successfully submitted to Automatic Speech Recognition (ASR) Slack Kaggle"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/data/11-785-s23-hw3p2\" + \"/test-clean/random_submission.csv\"\n",
        "df = pd.read_csv(data_dir)\n",
        "df.label = results\n",
        "df.to_csv('submission.csv', index = False)\n",
        "\n",
        "!kaggle competitions submit -c automatic-speech-recognition-asr-slack-kaggle -f submission.csv -m \"460\"\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10 (main, Feb 16 2023, 02:49:39) [Clang 14.0.0 (clang-1400.0.29.202)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bd385fe162c5ca0c84973b7dd5c518456272446b2b64e67c2a69f949ca7a1754"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0cb2f07411354324a8453e036e9e6381": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58c1fb614c694f70b0fb699187911f3b",
              "IPY_MODEL_5131854c8cbc42c8afe44d50ae0700a6"
            ],
            "layout": "IPY_MODEL_75bd759b8b10480aaf278f43202894b3"
          }
        },
        "34b56fb0bb7c4a0bb9aa38f631023f96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5131854c8cbc42c8afe44d50ae0700a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d99878f563145cda52d8b3f49112190",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_34b56fb0bb7c4a0bb9aa38f631023f96",
            "value": 0.04583319931813065
          }
        },
        "58c1fb614c694f70b0fb699187911f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_983121c6636841bc933ddab7d8c4415b",
            "placeholder": "​",
            "style": "IPY_MODEL_94150d4927e44770a6aa0e3095683764",
            "value": "0.001 MB of 0.030 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "6d99878f563145cda52d8b3f49112190": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75bd759b8b10480aaf278f43202894b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94150d4927e44770a6aa0e3095683764": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "983121c6636841bc933ddab7d8c4415b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
